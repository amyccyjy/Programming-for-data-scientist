---
title: "ST340 Assignment 2"
author: "Xintian Han 1909780, Runze Wang 1907544, Jingyuan Chen 2029628 "
date: "26/02/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(mvtnorm)
library(tidyverse)
library(gridExtra)
library(MASS)
```


# Question 1

(a)

$$
f(\mu_{1:K})=f(\mu_1,....,\mu_K)=\sum_{i=1}^n\sum_{k=1}^{K}\gamma_{ik}logp(x_i|\mu_k)
$$
$$
=\sum_{i=1}^n\sum_{k=1}^{K}\gamma_{ik}log\prod_{j=1}^{p}\mu_{kj}^{xij}(1-\mu_{kj})^{1-xij}
$$

We differentiate the equation wrt $\mu_{1:K}$:
$$
\frac{\partial}{\partial\mu_{1:K}}f(\mu_{1:K})=\frac{\partial}{\partial\mu_{kj}}\sum_{i=1}^n\sum_{k=1}^{K}\gamma_{ik}log\prod_{j=1}^{p}\mu_{kj}^{xij}(1-\mu_{kj})^{1-xij}
=\frac{\partial}{\partial\mu_{kj}}\sum_{i=1}^n\sum_{k=1}^{K}\sum_{j=1}^p\gamma_{ik}log\mu_{kj}^{xij}(1-\mu_{kj})^{1-xij}
$$

$$
=\frac{\partial}{\partial\mu_{kj}}\sum_{i=1}^n\gamma_{ik}log\mu_{kj}^{xij}(1-\mu_{kj})^{1-xij}
=\sum_{i=1}^n\gamma_{ik}(\frac{x_{ij}}{\mu_{kj}}-\frac{1-x_{ij}}{1-\mu_{kj}})=0
$$


$$
\sum_{i=1}^n\gamma_{ik}x_{ij}-\sum_{i=1}^n\gamma_{ik}x_{ij}\mu_{kj}=\mu_{kj}\sum_{i=1}^n\gamma_{ik}(1-x_{ij})
$$

$$
\mu_{kj}=\frac{\sum_{i=1}^n\gamma_{ik}x_{ij}}{\sum_{i=1}^n\gamma_{ik}}
$$
Therefore

$$
\mu_{k}=\frac{\sum_{i=1}^n\gamma_{ik}x_{i}}{\sum_{i=1}^n\gamma_{ik}}
$$
(bi)

In this part, we are going to use some part of the code from the lab4 and creating the mixture bernoulli distribution and the EM algorithm.
```{r}
load("~/Downloads/20newsgroups.rdata")
## EM algorithm for a mixture of multivariate Bernoulli 

# fix random number generator (seed)


## logsumexp(x) returns log(sum(exp(x))) but performs the computation in a more stable manner
logsumexp <- function(x) return(log(sum(exp(x - max(x)))) + max(x))


# we create the density of multivariate bernoulli distribution 
bernoulli <- function(a) {
  b <- prod(a)
  return(b)
}

#we can test the bernoulli function is actually working
bernoulli(c(0.4,0.6))
bernoulli(c(1,0))

# compute the log-likelihood 
compute_ll <- function(xs,mus,lws,gammas) {
  ll <- 0
  n <- dim(xs)[1]
  K <- dim(mus)[1]
  v1 <- rep(1,dim(xs)[2])
  for (i in 1:n) {
    for (k in 1:K) {
      if (gammas[i,k] > 0) {
        ll <- ll +
  gammas[i,k]*(lws[k]+log(bernoulli((mus[k,])^(xs[i,])*(v1-mus[k,])^(v1-xs[i,])))-log(gammas[i,k]))
      }
    }
  }
  return(ll)
}

# compute the log-likelihood directly
compute_ll.direct <- function(xs,mus,lws) {
  ll <- 0
  v1 <- rep(1,dim(xs)[2])
  n <- dim(xs)[1]
  K <- dim(mus)[1]
  for (i in 1:n) {
    s <- 0
    for (k in 1:K) {
      s <- s + exp(lws[k])*(bernoulli((mus[k,])^(xs[i,])*((v1-mus[k,])^(v1-xs[i,]))))
    }
    ll <- ll + log(s)
  }
  return(ll)
}

em_mix_bernoulli <- function(xs,K,max.numit=Inf) {
  p <- dim(xs)[2]
  n <- dim(xs)[1]
  v1 <- rep(1,dim(xs)[2])
  # lws is log(ws)
  # we work with logs to keep the numbers stable
  # start off with ws all equal
  lws <- rep(log(1/K),K)
  
  # start off with a random selection of cluster means as a matrix from uniform distribution
  mus <- matrix(data = runif(n = K * p), nrow = K, ncol = p)
  
  # gammas will be set in the first iteration 
  gammas <- matrix(0,n,K)
  
  converged <- FALSE
  numit <- 0
  ll <- -Inf
  print("iteration : log-likelihood")
  while(!converged && numit < max.numit) {
    numit <- numit + 1
    mus.old <- mus
    ll.old <- ll
    
    ## E step - calculate gammas
    for (i in 1:n) {
      # the elements of lprs are log(w_k * p_k(x)) for each k in {1,...K}
      lprs <- rep(0,K)
      for (k in 1:K) {
        lprs[k] <- lws[k] + log(bernoulli((mus[k,])^(xs[i,])*((v1-mus[k,])^(v1-xs[i,]))))
      }
      # gammas[i,k] = w_k * p_k(x) / sum_j {w_j * p_j(x)}
      gammas[i,] <- exp(lprs - logsumexp(lprs))
    }
    
    ll <- compute_ll(xs,mus,lws,gammas)
    # we could also compute the log-likelihood directly below
    # ll <- compute_ll.direct(xs,mus,Sigmas,lws)
    
    # M step - update ws, mus
    Ns <- rep(0,K)
    for (k in 1:K) {
      Ns[k] <- sum(gammas[,k])
      lws[k] <- log(Ns[k])-log(n)
      
      mus[k,] <- rep(0,p)
      
      for (i in 1:n) {
        mus[k,] <- mus[k,]+gammas[i,k]/Ns[k]*xs[i,]
        
        # potential non positive definite matrix and non simmetric
        # just a triangle is needed to computed, the rest is the mirror of it
      }
      
    }
    print(paste(numit,": ",ll))
    
    mus[which(mus > 1,arr.ind=TRUE)] <- 1 - 1e-15
    
    # we stop once the increase in the log-likelihood is "small enough"
    
    if (abs(ll-ll.old) < 1e-5) converged <- TRUE
  }
  
  
  
  return(list(lws=lws,mus=mus,gammas=gammas,ll=ll))
  
}


```

Then we run the algorithm for K=4, could get the following.
We finally observed that , after a large number of iterations , the log-likelihood converged.
```{r}
out10<- em_mix_bernoulli(documents,4)
```

b(ii)

We have K = 4,four clusters in the mixture bernoulli model.
We know that the value in the newsgroups data set is the true value and we use $\gamma_k$ to estimate the $\mu_k$. In this case using part(a), we could find the maximum value of $\gamma$ for each n where n=16242. Then we could perform a table for the true data set and the estimaated clusters. After that, we turned the table into a matrix for convenience in further calculations. We need to find the matrix element that has the maximum value both in each row and column, that wiil be the true value$=$estimated value for four clusters. Finally, we could get the total accuracy. 

```{r}
n<-16242
true_label <- as.vector(newsgroups) 
# we could get the maximum value of gamma 
cluster <- apply(out10$gammas, 1, which.max)

matrix.f<- function(true_label,cluster,k) {
   n<-16242
  table <- table(true_label,cluster) 
  matrix4 <- table
  accuracy<-0
   m_element <-numeric(k)
repeat {
  max_r <- apply(matrix4, 1, max)
  max_c <- apply(matrix4, 2, max)
  max_v <- max(matrix4)
  if (max_v == 0) {
    break
  }
  max_n <- which(matrix4 == max_v, arr.ind = TRUE)
  i <- max_n[,1]
  j <- max_n[,2]
  if (max_r[i] == max_v && max_c[j] == max_v) {
    m_element[i] <- max_v
    accuracy <- accuracy + max_v
    matrix4[i,] <- rep(0, k)
    matrix4[,j] <- rep(0, k)
  } else {
    matrix4[i,j] <- 0
  }
}

total_accuracy <- paste(accuracy/n)
output<- list(table = table, m_element = m_element, total_accuracy = total_accuracy)
return(output)}


```


```{r}
matrix.f(true_label,cluster,4)
```
The total accuracy is 0.63.

Also using the code below we actually find there is misallocation of the clusters with estimated value using the bar chart to iluustrate the frequency.
```{r}

as <- matrix.f(true_label, cluster, k = 4)
table <- as$table

# we could plot the frequency graph 
barplot(table, beside = TRUE,col = grey.colors(ncol(table)),
        main = "Cluster vs True_label", xlab = "Cluster",
        ylab = "Frequency", legend.text = rownames(table))
```
This graph shows the data performed for the estimated values and true values. From the graphs we can there is misallocation of data in clusters and true labels for all clusters, so they did not perform so well in this algorithm. As a result, the em algorithm did't perform  well in this data set, the reasons might be the optimal clusters do not match the four topic labels or clusters might not be reliable reproducible. To improve the accuracy,we could reallocated the values in order to get a more precise answer through permutation of tesing and assigning the true value to match the estimated value for four clusters.Otherwise, we could also find the words frequency in each cluster to check whether they correspond to the four board topics.



b(iii)
The sensitivity of the EM algorithm with respect to the initial parameters can be examined by comparing the results obtained from different initial values.
To reduce the sensitivity, it is common to run the EM algorithm multiple times with different initial values and choose the estimate that maximizes the likelihood or has the highest total accuracy value. 
We run the code for 3 times and obtained 3 different values for total accuracy for each time.
```{r}
#we repeat the process for 3 times 
accuracy_v <- replicate(3, {
  out10 <- em_mix_bernoulli(documents, 4)
  cluster <- apply(out10$gammas, 1, which.max)
  matrix.f(true_label, cluster, 4)$total_accuracy
})

print(accuracy_v)


```

With these output, we can say that the best performance has the highest accuracy value. we could also run the algorithm more than 3 times. In order to save time, we just use n=3 as an example.




# Question 2

(a)
Create a sampling function for thompson sampling (bernoulli distribution)
```{r}
## Set Bernoulli the success probability for each arm.
ps <- c(0.6,0.4)

arm <- function(ns,ss) {
## the number of successes a
  a <- 1 + ss 
## the number of failures b
  b <- 1 + ns - ss
  
## probabilities distribution of two arms by using Beta distributions
  L <- rbeta(1,a[1],b[1])
  R <- rbeta(1,a[2],b[2])
  
## If L is great than R, and we will pull the arm 1 next time, so the function returns 
## the value "1";if R is great than L, and we will choose the arm2 next time, so the 
## function returns the value "2"
  if (L > R) {
    return(1)
  } else {
    return(2)
  }
}

```

Create the Thompson sampling function
```{r}
th.be <- function(ps,n) {
## the values of arm played from time 1 to n
  as <- rep(0,n)
## the reward values from time 1 to n
  rs <- rep(0,n)
  
## initialization that the number of plays and number of successes is 0 
## for arm 1 and arm 2
  ns <- rep(0,2); ss <- rep(0,2)
  
  for (i in 1:n) {
    ## choose one of two arms
    a <- arm(ns,ss)
    
    ## If we get one random probability which is less than probability of arm 'a', 
    ## and r should be 1; if we the probability is great than the probability of 
    ## arm 'a', and r will be 0
    r <- ifelse(runif(1) < ps[a], 1, 0)
    ns[a] <- ns[a] + 1
    ss[a] <- ss[a] + r
    as[i] <- a
    rs[i] <- r
  }
  return(list(as=as,rs=rs))
}
```

Run one example of Thompson Sampling
```{r}
## Adding the success parameters and time
thompson.bernoulli.out0 <- th.be(ps=ps,n=100)

## The total reward value we get by using Thompson sampling 
sum(thompson.bernoulli.out0$rs)
```

Create the epsilon decreasing function $\epsilon(n)$. The decreasing function of $\epsilon$ with input the constant value $C$, the negative value $k$ and time $n$
```{r}
ep<-function(C,k,n){
  e<-rep(0,n)

  for(i in 1:n){
  e[i]<-min(1,C*i^k)
  }
  return(e)
}
```

Let $C$=1, $k$=-1 and $n$ =1000, we can see the $\epsilon$-decreasing function $\min\{1,i^{-1}\}$ is decreasing from this plot
```{r}
x<-seq(1,1000,1)
plot(x,ep(C=1,k=-1,1000),type="l",
ylab = "y=epsilon(i)=min{1,1/i}",
xlab="n",
main = "The epsilon-decreasing function against n")
```
Create the $\epsilon$-decreasing function with input $ps$, $n$, and $ep$
```{r}
epsilon.de <- function(ps,n,ep) {
  ## initialize the list of choosing arms and rewards
  as <- rep(0,n); rs <- rep(0,n)
  ## initialize the numbers of times and successes
  ns <- rep(0,2); ss <- rep(0,2)
  
  ## Play arm 1 and arm 2 once, record reward, and update number of successes and 
  ## number of plays
  for (i in 1:2) {
    a <- i
    r <- runif(1) < ps[a]
    ns[a] <- ns[a] + 1
    ss[a] <- ss[a] + r
    as[i] <- a
    rs[i] <- r
  }
  
  ## now follow the epsilon decreasing strategy from the time 3 because we have pulled 
  ## two arms from time 1 and 2
  for (i in 3:n) {
    
    ## with probability of epsilon decreasing function "ep" we have set, we can pick an 
    ## arm uniformly at random
   if (runif(1) < ep[i]) {
      a <- sample(2,1)
    } else { 
      ## if not, we will choose the best arm so far
      a <- which.max(ss/ns)
    }
    
     r <- ifelse(runif(1) < ps[a], 1, 0)
     
    ## update the numbers of plays and successes
     ns[a] <- ns[a] + 1
     ss[a] <- ss[a] + r
     
    ## record the arm played and the reward received
    as[i] <- a
    rs[i] <- r
  }
  
  ## print out the record the armed played and the reward received totally
  return(list(as=as,rs=rs))
}
```

(b)

In the part b, the $\epsilon$-Greedy is a simple method to balance exploration and exploitation by choosing between exploration and exploitation randomly. We can select an initial $epsilon$ value using the $\epsilon$-decreasing strategy that will gradually drop over time. This makes sure that there is a lot of exploration in the early stages of the experiment, but as time goes on and the best choice becomes more certain, there are fewer iterations dedicated to making less-than-ideal choices.

At first, we will play each arm once, record reward, and update number of successes and number of plays. From time 3 , with prob $\epsilon(3)$, play an arm at random; with prob of $1-\epsilon(3)$, play the one with the best rate of success so far. The $\epsilon_{n}=\min\{1,Cn^{-1}\}$, and this function is decreasing, so the return value of $\epsilon$ is decreasing as the time goes up. This can increasing the prob of choosing the best rate of success so far, so that the value of $1-\epsilon(n)$ is almost close to 1. Now we have $\mu1 > \mu2$, and $\exp(sum(r_1:r_N)/N)$ should be close to our best arm$\mu1$ which is 0.6 in our question.

Run $epsilon.de$ with the given $ps$ and $\epsilon_{n}=\min\{1,Cn^{-1}\}$ for one time, $C$ is a positive constant and $k$ should be negative value 
```{r}
## One simple example for 10000 iteration and only 1 run
egd.out1 <- epsilon.de(ps=ps,n=1e4,ep(C=1,k=-1,n=1e4))
sum(egd.out1$rs)
sum(egd.out1$rs)/length(egd.out1$rs)
```

We need to run for $N$ times so that we can get the average rewards, so the following function is based
on $N$ runs , success probabilities $ps$, $n$ iterations(times), positive constant $C$, and negative value $k$.
```{r}
## Here we create a new function to test epsilon-decreasing algorithm for N runs and n iterations 
## and get the value of n rewards

epsilon.t<-function(N,ps,n,C,k){
  reward<-rep(0,n)
  for (i in 1:N) {
    
    ## we can get the reward for each time and accumulate these rewards as the time increases
    rs<-(epsilon.de(ps,n,ep(C,k,n)))$rs
    reward<-reward+(1/N)*(cumsum(rs)/(1:n))
  }
  return(reward)
}
```

Comparison with different values of constant $C$, and control other conditions same
```{r epsilont, warning = FALSE}
## With the reward test function, we can change the value of positive constant C, and 
## keep other conditions constant, and in this part k=-1
  t1<-epsilon.t(100,ps=c(0.6,0.4),10000,1,-1)
  t2<-epsilon.t(100,ps=c(0.6,0.4),10000,5,-1)
  t3<-epsilon.t(100,ps=c(0.6,0.4),10000,10,-1)

## Using plot function to plot the average reward against time n between three different 
## curves and the probability of our best arm of 0.6
plot(n=c(1:10000),t1,type="l",col="green",ylim=c(0.5,0.62),xlab="n",ylab="average rewards",
       main ="Three curves for different constant C with k=-1" )
  lines(n=c(1:10000),t2,type="l",col="blue")
  lines(n=c(1:10000),t3,type="l",col="red")
  abline(h=0.6,col="gray")
  legend('bottom', col=c("green","blue", "red"),
       lty=c(1,1), lwd=c(2,2), 
       legend=c("C=1", "C=5","C=10"))

```
Therefore, the plot shows that the higher $C$ can converge to 0.6 faster than smaller $C$, because smaller $C$ need to run more times than bigger $C$. The $\epsilon -decreasing$ strategy defined with $\epsilon_n = min\{1, Cn^{-1}\}$ is asymptotically optimal as our graphs with different values of $C$ can converge to 0.6 eventually.

(c)
$epsilon.de$ with the given $ps$ and $\epsilon{n}= \min\{ 1, Cn^-2\}$ and $C$ is a positive constant
```{r}
## One simple example for 1e4 iteration and only 1 run
egd.out <- epsilon.de(ps=ps,n=1e4,ep(C=1,k=-2,n=1e4))
sum(egd.out$rs)
sum(egd.out$rs)/length(egd.out$rs)
```

Comparison with different values of constant $C$, and control other conditions same
```{r warning=FALSE}
## In this part, we will run for 100 times and 10000 iterations for getting the average reward.
  t1<-epsilon.t(100,ps=c(0.6,0.4),10000,1,-2)
  t2<-epsilon.t(100,ps=c(0.6,0.4),10000,5,-2)
  t3<-epsilon.t(100,ps=c(0.6,0.4),10000,10,-2)

plot(n=c(1:10000),t1,type="l",ylim=c(0.5,0.62),col="green",xlab="n",ylab="average rewards",
       main ="Three curves for different constant C with k=-2" )
  lines(n=c(1:10000),t2,type="l",col="blue")
  lines(n=c(1:10000),t3,type="l",col="red")
  abline(h=0.6,col="gray")
  legend('bottom', col=c("green","blue", "red"),
       lty=c(1,1), lwd=c(2,2), 
       legend=c("C=1", "C=5","C=10"))
  
```
The $\epsilon -decreasing$ strategy defined with $\epsilon_n = min\{1, Cn^{-2}\}$ is not asymptotically optimal because our graphs with the highest and smallest $C$ cannot converge to 0.6 eventually.

(d) 
part(i) Compare graph of the implementations of $\epsilon$-decreasing and Thompson sampling for this problem
```{r}
## Create a Thompson Sampling reward function 
Thompson.t<-function(N,ps,n){
  reward<-rep(0,n)
  for (i in 1:N) {
    #record the reward
    rs<-(th.be(ps,n))$rs
    #accumulate these rewards
    reward<-reward+(1/N)*(cumsum(rs)/(1:n))
  }
  return(reward)
}
```

Compare two curves of asymptotically optimal $\epsilon$-decreasing and Thompson Sampling
```{r warning=FALSE}
## Using epsilon-decreasing function that epsilon(i)=min{1,1/i}, and compare this function 
## with the Thompson Sampling of the same runs and lengths
e<-epsilon.t(100,ps=c(0.6,0.4),10000,1,-1)
t<-Thompson.t(100,ps=c(0.6,0.4),10000)

plot(n=c(1:10000),e,type="l",ylim=c(0.46,0.62),col="blue",xlab="n",ylab="average rewards",
       main ="Two curves for epsilon-decreasing and Thompson Sampling" )
  lines(n=c(1:10000),t,type="l",col="red")
  abline(h=0.6,col="gray")
  legend('bottom', col=c("blue","red"),
       lty=c(1,1), lwd=c(2,2), 
       legend=c("Epsilon-decreasing", "Thompson Sampling"))
  
```
From this plot, we can see that Thompson Sampling can converge faster to 0.6 than the $\epsilon$-decreasing. So we suppose that Thompson Sampling is better than the $\epsilon$-deceasing.

(d)
part(ii)For two asymptotically optimal procedures, there is a bound on how good any procedure can be in terms of regret: Expected lost reward (From the lecture notes). \[
R(n)=\exp[sum_{i=1}^{n}(\mu_1-R_i)]
\]  and our best arm probability $\mu_1 =$ 0.6 in our question. From the Optimal asymptotic regret of Thompson Sampling with $\mu_1$$>$$\mu_2$, 

we can get \[
\boldsymbol{R(n)}\le(1+\epsilon)\frac{\mu_1-\mu_2}{D_{KL}(mu_2||mu_1)}log(n)+O_{\epsilon,\mu_1,\mu_2}(log(n))
\] (From the lecture notes)
for any $\epsilon$ > 0 and $\mu_1= 0.6$, and $\mu_2=0.4$
```{r}
## Calculate D_kl constant 
D_kl<-0.2/(0.6*log(6/4)+0.4*log(4/6))
D_kl
```
Therefore we can get \[
\frac{\mu_1-\mu_2}{D_{KL}(mu_2||mu_1)}=\frac{0.6-0.4}{D_{KL}(0.6||0.4)} =\frac{0.2}{0.6log\frac{0.6}{0.4}+(1-0.6)log\frac{1-0.6}{1-0.4}}=2.466303
\] and  we take this constant value into the plot of "realized regret"/log(n) against n
```{r}
n<-1e5
## Create the expected lost reward using the above equations for epsilon-decreasing
ep.r<-ps[1]*(1:n)-cumsum((epsilon.de(ps,n,ep(C=1,-1,n)))$rs)
ep.Rn<-ep.r/log(1:n)
## Create the expected lost reward using the above equations for Thompson Sampling
th.r<-ps[1]*(1:n)-cumsum((th.be(ps,n))$rs)
th.Rn<-th.r/log(1:n)
```

Plot the graph of regret/log(n) against n and the constant line \[
\frac{\mu_1-\mu_2}{D_{KL}(mu_2||mu_1)}=2.466303
\]
```{r warning=FALSE}
## Plot two curves and constant
plot(n=c(1:100000),ep.Rn,ylim=c(-30,30),col="blue",type="l",xlab="n",ylab="'realized regret'/log(n)")
  lines(n=c(1:100000),th.Rn,col="red",type="l")
  abline(h=2.466303)
  legend('topleft', col=c("blue","red"),
       lty=c(1,1), lwd=c(2,2), cex=0.6,
       legend=c("Epsilon-decreasing", "Thompson Sampling"))
```
From this plot, it shows that Thompson sampling has smaller regret than $\epsilon$-decreasing with the same times $n$ , a more stable $R(n)/log(n)$, and we have seen that Thomspon Sampling can converge faster than $\epsilon$, so Thompson Sampling is better than $\epsilon$-decreasing strategy. We can usually choose the Thompson Sampling in this problem.



# Question 3

(a)
we know that using the inverse-distance weighting we could get the predicted values as :
$$
\frac{\sum_{i=1}^kw_iy_i}{\sum_{i=1}^kw_i}    
$$
with
$$
  w_i=\frac{1}{k_i}
$$

 
```{r}
knn.regression.test <- function(k, train.X, train.Y, test.X, test.Y, distances) {
 # calculate the distance between trainX and testX 
  distances <- distances(train.X,test.X)
 # the weighted sum 
 weight_Y <- rep(0,length(test.X[,1]))
weight <- rep(0,length(test.X[,1]))
  for(i in 1:length(test.X[,1])){
  # sort in increasing order 
   sort <- sort(distances[,i])
   # for each of the KNN
    for(x in 1:k){
     weight_Y[i] <- weight_Y[i] + (1/sort[x])*(train.Y[match(sort[x],distances[,i])])
      weight[i] <- weight[i] + (1/sort[x])
    }
  }
  estimates <- weight_Y/weight
 print(sum((test.Y - estimates)^2))
}
```


(b)

Toy data 1
```{r cars}
distances.l1 <- function(X,W) {
  apply(W,1,function(p) apply(X,1,function(q) sum(abs(p-q))))
}
n <- 100
set.seed(2021)
train.X <- matrix(sort(rnorm(n)),n,1)
train.Y <- (train.X < -0.5) + train.X*(train.X>0)+rnorm(n,sd=0.03)
plot(train.X,train.Y)
test.X <- matrix(sort(rnorm(n)),n,1)
test.Y <- (test.X < -0.5) + test.X*(test.X>0)+rnorm(n,sd=0.03)
k <- 2
knn.regression.test(k,train.X,train.Y,test.X,test.Y,distances.l1)



```
we could get different value for k from 1 to 50:
```{r}
total_square_error<- rep(0,50)
for(i in 1:50) {
total_square_error[i] <- knn.regression.test(i,train.X,train.Y,test.X,test.Y,distances.l1)
}
plot(total_square_error,xlab="k",pch=16)

```

From the graph we can see that for k$>$8 total_square_error values increase linearly as k increases, for $2\le k\le8$ total_square_error values decrease linearly as k increases.




```{r}
min_index <- which.min(total_square_error)
min_value <- total_square_error[min_index]
min_index  
min_value

```
The minimum total_square_error is 2.586138 when k=8







Toy data 2

```{r}

set.seed(100)
train.X <- matrix(rnorm(200),100,2)
train.Y <- train.X[,1]
test.X <- matrix(rnorm(100),50,2)
test.Y <- test.X[,1]
k <- 3
knn.regression.test(k,train.X,train.Y,test.X,test.Y,distances.l1)
```

we could get different value for k from 1 to 50:
```{r}
total_square_error<- rep(0,50)
for(i in 1:50) {
total_square_error[i] <- knn.regression.test(i,train.X,train.Y,test.X,test.Y,distances.l1)
}
plot(total_square_error,xlab="k",pch=16)

```
From the graph we see tnat for 1$\le$ k total_square_error values increase linearly as k increases.
```{r}
min_index <- which.min(total_square_error)
min_value <- total_square_error[min_index]
min_index  
min_value
```
The minimum total_square_error is 3.072546 when k=1


Oevrall from both graphs we see that as k increases,the total_square_error shows an approximately linear relationship with k

(c)
We use tibble to show years and predictions combined with part (a) 
```{r}
Estimates<- function(k, train.X, train.Y, test.X, test.Y, distances) {
 distances <- distances(train.X,test.X)
 weight_Y <- rep(0,length(test.X[,1]))
  weight <- rep(0,length(test.X[,1]))
  for(i in 1:length(test.X[,1])){
    sort <- sort(distances[,i])
     for(x in 1:k){
      weight_Y[i] <- weight_Y[i] + (1/sort[x])*(train.Y[match(sort[x],distances[,i])])
      weight[i] <- weight[i] + (1/sort[x])
    }
  }
  return( weight_Y/weight)
}
distances.l2 <- function(X,W) {
apply(W,1,function(p) apply(X,1,function(q) sqrt(sum((p-q)^2))))
}
Iowa <- read.delim("~/Downloads/Iowa.txt")
train.X=as.matrix(Iowa[seq(1,33,2),1:9])
train.Y=c(Iowa[seq(1,33,2),10])
test.X=as.matrix(Iowa[seq(2,32,2),1:9])
test.Y=c(Iowa[seq(2,32,2),10])
k <- 5
tibble(year = test.X[,1]) %>%
  mutate(prediction =Estimates(k,train.X,train.Y,test.X,test.Y,distances.l2))
```


(d)


ordinary least squares
```{r}
Iowa <- read.delim("~/Downloads/Iowa.txt")
train <- Iowa[seq(1,33,2),]
test.X <- Iowa[seq(2,32,2),1:9]
test.Y <- Iowa[seq(2,32,2),10]
ols_pridiction <- predict(lm(Yield~., data = train), newdata = test.X)
sum((test.Y - ols_pridiction)^2)
ols_error<-sum((test.Y - ols_pridiction)^2)
ols_error

```
Ridge regression
```{r}
train <- Iowa[seq(1,33,2),]
test.X <- Iowa[seq(2,32,2),1:9]
test.Y <- Iowa[seq(2,32,2),10]
ridge_regression <-lm.ridge(Yield~ ., data = train, lambda = 10)
ridge_pridiction <- as.matrix(cbind(const=1, test.X)) %*% coef(ridge_regression) 
ridge_error<- sum((test.Y - ridge_pridiction)^2)
ridge_error
```

```{r}
total_square_error<- rep(0,20)
for(i in 1:20) {
total_square_error[i] <- knn.regression.test(i,train.X,train.Y,test.X,test.Y,distances.l2)
}

   plot(c(1:20),total_square_error,xlab="k",ylab="'square errors",pch=16)
    abline(h=ols_error,col="blue")
    abline(h=ridge_error,col="green")
    legend('bottomright', col=c("blue","green"),
       lty=c(1,1),lwd=c(1,1),cex=0.6,
       legend=c("ordinary least squares", "Ridge regression"))
   
```
The blue line is ordinary least squares model and the green line is Ridge regression model.The ordinary least squares model is greater than Ridge regression model.The lowest point in Knn is less than Ridge regression model,but large proportion of the points lies between least squares model and Ridge regression model.So the estimation for Ridge regression model and Knn are better than the estimation for ordinary least squares model.The estimation of Ridge regression model and Knn are depend on the specific value of K in general case.



